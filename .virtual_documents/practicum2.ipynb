import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Load the data
bird_df = pd.read_csv("Bird_Strikes_1990_2023.csv")

bird_df


# Data Prep 1: Convert data types
bird_df['INCIDENT_DATE'] = pd.to_datetime(bird_df['INCIDENT_DATE'])
bird_df['TIME'] = pd.to_datetime(bird_df['TIME'], errors='coerce')
bird_df['REPORTED_TITLE'] = bird_df['REPORTED_TITLE'].astype('string')
bird_df['SOURCE'] = bird_df['SOURCE'].astype('string')
bird_df['PERSON'] = bird_df['PERSON'].astype('string')
bird_df['LUPDATE'] = pd.to_datetime(bird_df['LUPDATE'])
print("\nData types of each column after converting:")
print(bird_df.dtypes)

# Data Prep 2: Handle missing values (drop only if multiple fields missing)
print("\nMISSING VALUES BEFORE:")
useful_fields = ['SPEED', 'COST_REPAIRS_INFL_ADJ', 'SKY', 'PRECIPITATION', 'NUM_SEEN']

for field in useful_fields:
    if field in bird_df.columns:
        missing = bird_df[field].isnull().sum()
        print(f"{field:25s}: {missing:,} missing")


# Drop rows only if 3 or more of these 5 fields are missing
# thresh=3 means keep rows with at least 3 non-null values out of 5
bird_df = bird_df.dropna(subset=useful_fields, thresh=3)


# Data Prep 3: Handle unknown values
# Find and impute all columns with "Unknown" values
columns_with_unknown = []
for col in bird_df.columns:
    if bird_df[col].astype(str).str.contains('Unknown', case=False, na=False).any():
        unknown_count = (bird_df[col].astype(str).str.lower() == 'unknown').sum()
        if unknown_count > 0:
            columns_with_unknown.append(col)
            print(f"{col}: {unknown_count:,} unknown values")


for col in columns_with_unknown:
    # Get most common value (excluding Unknown)
    non_unknown = bird_df[bird_df[col].astype(str).str.lower() != 'unknown'][col]

    if len(non_unknown) > 0:
        most_common = non_unknown.mode()[0]

        # Replace Unknown (case insensitive)
        bird_df[col] = bird_df[col].replace(['Unknown', 'unknown', 'UNKNOWN'], most_common)

        print(f"{col}: Replaced with '{most_common}'")
































from scipy import stats
from scipy.stats import chi2_contingency
bird_df.columns


bird_df.shape


plt.scatter(bird_df["SPEED"], bird_df["HEIGHT"])

plt.xlim(0, 410)
plt.xlabel("Aircraft Speed (Knots)")
plt.ylabel("Aircraft Height (Feet Above Ground Level)")
plt.title("Scatterplot of Aircraft Speed vs. Height")

plt.show()





correlation_coeff = bird_df['SPEED'].corr(bird_df['HEIGHT'])
print(f"The correaltion coefficient is: {correlation_coeff}")





bird_df["PHASE_OF_FLIGHT"].unique()


approach_phase_df = bird_df[bird_df["PHASE_OF_FLIGHT"] == "Approach"]
arrival_phase_df = bird_df[bird_df["PHASE_OF_FLIGHT"] == "Arrival"]

# Due to the NaN values in both, i will be replacing them with the average

mean_approach_speed = approach_phase_df['SPEED'].mean()
mean_arrival_speed = arrival_phase_df['SPEED'].mean()

approach_phase_df['SPEED'].fillna(mean_approach_speed, inplace=True)
arrival_phase_df['SPEED'].fillna(mean_arrival_speed, inplace=True)


# Plots of the different flight phases distribution (histrogram)
fig, (ax1, ax2) = plt.subplots(1, 2) 
plt.figure(figsize=(10, 5))

fig.suptitle('Distribution of Flight Phase Speeds')

ax1.hist(approach_phase_df["SPEED"], bins=50, edgecolor='black')

ax1.set_title('Approach')
ax1.set(xlabel='Aircraft Speed (Knots)', ylabel='Frequency')

ax2.hist(arrival_phase_df["SPEED"], bins=50, edgecolor='black')

ax2.set_title('Arrival')
ax2.set(xlabel='Aircraft Speed (Knots)')

plt.show()


# two-sample t-test
approach_phase_speeds = approach_phase_df['SPEED']
arrival_phase_speeds = arrival_phase_df['SPEED']

t_stat, p_value = stats.ttest_ind(approach_phase_speeds, arrival_phase_speeds, equal_var=False)

print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")





# Aircraft damage
bird_df["DAMAGE_LEVEL"].fillna('Unknown', inplace=True)
bird_df['DAMAGE_LEVEL'] = bird_df['DAMAGE_LEVEL'].replace({'N': 'None', 'M': 'Minor', 'M?': 'Undetermined level', 'S': "Substantial", 'D':'Destroyed'})

grouped_aircraft_dmg = bird_df.groupby(["PHASE_OF_FLIGHT"])["DAMAGE_LEVEL"].value_counts()
grouped_aircraft_dmg


# creating a stacked bar chart for grouped data
stacked_data = grouped_aircraft_dmg.unstack(fill_value=0)
stacked_data.plot(kind='bar', stacked=True, figsize=(12, 6), 
                  colormap='RdYlGn_r',
                  title='Stacked Damage Levels by Flight Phase')
plt.xlabel('Flight Phase')
plt.ylabel('Incident Count for Damage')
plt.xticks(rotation=0)

plt.legend(title='Damage Level')
plt.tight_layout()
plt.show()








# Creating a contingency table
contingency_table = pd.crosstab(
    bird_df['PHASE_OF_FLIGHT'], 
    bird_df['DAMAGE_LEVEL']
)


# performing the chi-squared 
chi2, p_value, dof, expected_frequencies = chi2_contingency(contingency_table)

print(f"\nChi-Squared Statistic: {chi2: .3f}")
print(f"P-value: {p_value: .3f}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies:")






bird_df.columns


# Planning to run a two sample z-test for this population: Comparing damage rates between small vs large aircraft
from statsmodels.stats.proportion import proportions_ztest

small_aircraft = bird_df[bird_df['AC_MASS'] == 1]  # ≤2,250 kg
large_aircraft = bird_df[bird_df['AC_MASS'].isin([4, 5])]  # >27,000 kg

# Calculate damage proportions
p1 = (small_aircraft['DAMAGE_LEVEL'].isin(['S', 'D'])).mean()  # Severe damage rate
n1 = len(small_aircraft)
p2 = (large_aircraft['DAMAGE_LEVEL'].isin(['S', 'D'])).mean()
n2 = len(large_aircraft)

# Two-sample z-test for proportions
counts = [p1*n1, p2*n2]
nobs = [n1, n2]
z_stat, p_value = proportions_ztest(counts, nobs)
print(f"Damage rate small vs large: z = {z_stat:.3f}, p = {p_value:.4f}")








import plotly.express as px


# value counts to find bird_strikes by state
state_df = (bird_df['STATE'].value_counts().reset_index(name='bird_strikes'))
state_df


# Creating a USA heatmap using plotly choropleth
# https://plotly.com/python/choropleth-maps/
fig = px.choropleth(
    state_df,
    locations='STATE',
    locationmode="USA-states",
    color='bird_strikes',
    color_continuous_scale="viridis",
    scope="usa",
    title='Number of Bird Strikes by U.S. State',
    labels={'bird_strikes': 'Bird Strikes'}
)

# Display the map
fig.show()





bird_df.columns


pts = bird_df.copy()

pts['LATITUDE']  = pd.to_numeric(pts['LATITUDE'], errors='coerce')
pts['LONGITUDE'] = pd.to_numeric(pts['LONGITUDE'], errors='coerce')
pts = pts.dropna(subset=['LATITUDE','LONGITUDE'])

# (optional) keep roughly-CONUS bounds to remove obvious outliers
pts = pts.query('-170 <= LONGITUDE <= -50 and 15 <= LATITUDE <= 72')

# tidy hover fields that exist in your columns
hover_cols = [
    'INCIDENT_DATE', 'AIRPORT', 'STATE', 'PHASE_OF_FLIGHT',
    'SPECIES', 'DAMAGE_LEVEL', 'COST_REPAIRS', 'EFFECT'
]

hover_cols = [c for c in hover_cols if c in pts.columns] 

# --- map ---
fig = px.scatter_map(
    pts,
    lat='LATITUDE',
    lon='LONGITUDE',
    color=pts['PHASE_OF_FLIGHT'].fillna('Unknown') if 'PHASE_OF_FLIGHT' in pts else None,
    hover_data=hover_cols,
    zoom=3,
    height=650,
    title='Bird Strike Incidents — clustered points'
)

# clustering points for better general areas
fig.update_traces(cluster=dict(enabled=True))

fig.update_layout(map_style="carto-positron")
fig.show()



